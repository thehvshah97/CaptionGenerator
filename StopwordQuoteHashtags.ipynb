{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog is running through the grass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descfile=open('descfile.txt','r')\n",
    "for line in descfile:\n",
    "    desc=' '.join(line.split(' ')[1:-1])\n",
    "print(desc)\n",
    "mainfile=open('mainfile.txt','w')\n",
    "mainfile=open('mainfile.txt','a')\n",
    "mainfile.write('**Description**\\n\\n')\n",
    "mainfile.write(desc+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'running', 'grass']\n"
     ]
    }
   ],
   "source": [
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english'))\n",
    "numarr=['one','two','three','four','five','six','seven','eight','nine','ten']\n",
    "words = desc.split() \n",
    "newarray=[]\n",
    "for r in words:\n",
    "    if not r in stop_words and not r in numarr: \n",
    "        newarray.append(r)\n",
    "print(newarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def getQuotes(keyword, numpages=2):\n",
    "    # Initialize lists\n",
    "    quoteArray = []\n",
    "    authorArray = []\n",
    "    pageNameArray = [keyword]\n",
    "    for i in range(2,numpages+1):\n",
    "        pageNameArray.append(keyword + \"_\" + str(i))\n",
    "    for page in pageNameArray:\n",
    "        # Obtain BrainyQuote page html\n",
    "        base_url = \"http://www.brainyquote.com/quotes/keywords/\"\n",
    "        url = base_url + keyword + \".html\"\n",
    "        response_data = requests.get(url).text[:]\n",
    "        soup = BeautifulSoup(response_data, 'html.parser')\n",
    "        # Populate quoteArray\n",
    "        for item in soup.find_all('a', class_='b-qt'):\n",
    "            quoteArray.append(item.get_text().rstrip())\n",
    "        # Populate authorArray\n",
    "        for item in soup.find_all('a', class_='bq-aut'):\n",
    "            authorArray.append(item.get_text())\n",
    "    # Create list of tuples of the form (quote, author)\n",
    "    ans1 = quoteArray\n",
    "    ans2 = authorArray\n",
    "    return ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 > No matter how you're feeling, a little dog gunna love you.  ->  Waka Flocka Flame \n",
      "\n",
      "\n",
      "2 > We're running the most dangerous experiment in history right now, which is to see how much carbon dioxide the atmosphere... can handle before there is an environmental catastrophe.  ->  Elon Musk \n",
      "\n",
      "\n",
      "3 > I believe a leaf of grass is no less than the journey-work of the stars.  ->  Walt Whitman \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "mainfile.write('\\n\\n**Quotes with Author**\\n\\n')\n",
    "for i in range(len(newarray)):\n",
    "    quoteArray, authorArray = getQuotes(newarray[i])\n",
    "    x=random.randint(1,len(quoteArray)-1)\n",
    "    quoteSelected=quoteArray[x]\n",
    "    authOfQuote=authorArray[x]\n",
    "    print(i+1,'>',quoteSelected, ' -> ',authOfQuote,'\\n\\n')\n",
    "    mainfile.write(str(str(i+1)+'> '+quoteSelected+' -> '+authOfQuote)+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting popular hashtags\n",
    "HashtagArray=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All relevant hashtags:\n",
      "\n",
      " ['#dog', '#dogsofinstagram', '#dogs', '#love', '#cute', '#puppy', '#doglover', '#hundefoto', '#beagle', '#bernesemontanhes', '#pet', '#dogoftheday', '#travel', '#puppylove', '#life', '#doglovers', '#dogstagram', '#hundeliebe', '#instadog', '#pom', '#pomeranian', '#like', '#nature', '#hundeaufinstagram', '#talenteddog', '#instagood', '#hundefotografie', '#instagram', '#pitbull', '#bhfyp', '#running', '#run', '#fitness', '#runner', '#training', '#gym', '#workout', '#runners', '#fit', '#motivation', '#instarunners', '#marathon', '#sport', '#trailrunning', '#runningmotivation', '#k', '#runnersofinstagram', '#fitnessmotivation', '#triathlon', '#instarun', '#instarunner', '#runhappy', '#cardio', '#crossfit', '#nike', '#love', '#garmin', '#laufen', '#fitfam', '#bhfyp', '#grass', '#nature', '#green', '#spring', '#photography', '#photooftheday', '#instagood', '#sun', '#sky', '#landscape', '#field', '#trees', '#naturephotography', '#flowers', '#love', '#tree', '#cute', '#flower', '#outdoors', '#summer', '#garden', '#ball', '#dog', '#photo', '#water', '#beautiful', '#farm', '#lawncare', '#pass', '#bhfyp'] \n",
      "\n",
      "\n",
      "Picked Hashtags:\n",
      "\n",
      " ['#run', '#bhfyp', '#crossfit', '#outdoors', '#hundefoto', '#runner', '#beagle', '#talenteddog', '#fitnessmotivation', '#life', '#motivation', '#trailrunning', '#instagram', '#sport', '#photooftheday', '#instarun'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting relevant hashtags\n",
    "temp=''\n",
    "temparr=[]\n",
    "mainfile.write('\\n\\n**Hashtags**\\n\\n')\n",
    "for tag in newarray:\n",
    "    rel_hash_url='http://best-hashtags.com/hashtag/'\n",
    "    rel_hash_url+=tag+'/'\n",
    "    try:\n",
    "        response_data = requests.get(rel_hash_url).text[:]\n",
    "        soup = BeautifulSoup(response_data, 'html.parser')\n",
    "        for item in soup.find_all('p1'):\n",
    "            temp+=' '+item.get_text().rstrip().strip()\n",
    "    except:\n",
    "        continue\n",
    "temparr=temp.split(' ')\n",
    "print('All relevant hashtags:\\n\\n',temparr[1:],'\\n\\n')\n",
    "for i in range(20):\n",
    "    newtag=temparr[random.randint(1,len(temparr)-1)]\n",
    "    if newtag in HashtagArray:\n",
    "        continue\n",
    "    else:\n",
    "        HashtagArray.append(newtag)\n",
    "print('Picked Hashtags:\\n\\n',HashtagArray,'\\n\\n')\n",
    "mainfile.write(' , '.join(HashtagArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
